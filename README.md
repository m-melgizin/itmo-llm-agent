# FastAPI Baseline Service
Это пример базового приложения, которое реализует API для обработки запросов и возврата ответов. Приложение написано на FastAPI и разворачивается с использованием Docker Compose. Сервис поддерживает интеграцию с различными языковыми моделями (LLM) и поисковыми системами для обработки запросов и генерации ответов.

## Основные возможности
- **Интеграция с языковыми моделями**: Поддержка моделей Llama, Gemini и DeepSeek через OpenRouter API.
- **Поиск информации**: Использование DuckDuckGo для поиска релевантной информации по запросу.

## Сборка
Для запуска выполните команду:

```bash
docker-compose up -d
```
Она соберёт Docker-образ, а затем запустит контейнер.

После успешного запуска контейнера приложение будет доступно на http://localhost:8080.

## Проверка работы
Отправьте POST-запрос на эндпоинт /api/request. Например, используйте curl:

```bash
curl --location --request POST 'http://localhost:8080/api/request' \
--header 'Content-Type: application/json' \
--data-raw '{
  "query": "В каком городе находится главный кампус Университета ИТМО?\n1. Москва\n2. Санкт-Петербург\n3. Екатеринбург\n4. Нижний Новгород",
  "id": 1
}'
```
В ответ вы получите JSON вида:

```json
{
  "id": 1,
  "answer": 1,
  "reasoning": "Из информации на сайте",
  "sources": [
    "https://itmo.ru/ru/",
    "https://abit.itmo.ru/"
  ]
}
```

id будет соответствовать тому, что вы отправили в запросе,
answer (в базовой версии) всегда будет 5.
## Кастомизация
Чтобы изменить логику ответа, отредактируйте функцию handle_request в main.py.
Если нужно использовать дополнительные библиотеки, добавьте их в requirements.txt и пересоберите образ.


Чтобы остановить сервис, выполните:

```bash
docker-compose down
```
